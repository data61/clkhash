{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Python API\n",
    "\n",
    "For this tutorial we are going to process a data set for private linkage with `clkhash` using the Python API.\n",
    "\n",
    "The Python package `recordlinkage` has a [tutorial](http://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html) linking data sets in the clear, we will try duplicate that in a privacy preserving setting.\n",
    "\n",
    "First install the dependencies we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "!pip install -U clkhash anonlink recordlinkage pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:12.026818499Z",
     "start_time": "2023-05-17T03:40:11.855049041Z"
    }
   },
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "import io\n",
    "import itertools\n",
    "import tempfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:12.087008521Z",
     "start_time": "2023-05-17T03:40:12.025850795Z"
    }
   },
   "outputs": [],
   "source": [
    "import clkhash\n",
    "from clkhash import clk\n",
    "from clkhash.field_formats import *\n",
    "from clkhash.schema import Schema\n",
    "from clkhash.comparators import NgramComparison\n",
    "from clkhash.serialization import serialize_bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:12.467801767Z",
     "start_time": "2023-05-17T03:40:12.087521763Z"
    }
   },
   "outputs": [],
   "source": [
    "from recordlinkage.datasets import load_febrl4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First load the dataset, and preview the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:12.512264268Z",
     "start_time": "2023-05-17T03:40:12.468192159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             given_name   surname street_number            address_1  \\\nrec_id                                                                 \nrec-1070-org   michaela   neumann             8       stanley street   \nrec-1016-org   courtney   painter            12    pinkerton circuit   \nrec-4405-org    charles     green            38  salkauskas crescent   \nrec-1288-org    vanessa      parr           905       macquoid place   \nrec-3585-org    mikayla  malloney            37        randwick road   \n\n                      address_2            suburb postcode state  \\\nrec_id                                                             \nrec-1070-org              miami     winston hills     4223   nsw   \nrec-1016-org         bega flats         richlands     4560   vic   \nrec-4405-org               kela             dapto     4566   nsw   \nrec-1288-org  broadbridge manor     south grafton     2135    sa   \nrec-3585-org            avalind  hoppers crossing     4552   vic   \n\n             date_of_birth soc_sec_id  \nrec_id                                 \nrec-1070-org      19151111    5304218  \nrec-1016-org      19161214    4066625  \nrec-4405-org      19480930    4365168  \nrec-1288-org      19951119    9239102  \nrec-3585-org      19860208    7207688  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>given_name</th>\n      <th>surname</th>\n      <th>street_number</th>\n      <th>address_1</th>\n      <th>address_2</th>\n      <th>suburb</th>\n      <th>postcode</th>\n      <th>state</th>\n      <th>date_of_birth</th>\n      <th>soc_sec_id</th>\n    </tr>\n    <tr>\n      <th>rec_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rec-1070-org</th>\n      <td>michaela</td>\n      <td>neumann</td>\n      <td>8</td>\n      <td>stanley street</td>\n      <td>miami</td>\n      <td>winston hills</td>\n      <td>4223</td>\n      <td>nsw</td>\n      <td>19151111</td>\n      <td>5304218</td>\n    </tr>\n    <tr>\n      <th>rec-1016-org</th>\n      <td>courtney</td>\n      <td>painter</td>\n      <td>12</td>\n      <td>pinkerton circuit</td>\n      <td>bega flats</td>\n      <td>richlands</td>\n      <td>4560</td>\n      <td>vic</td>\n      <td>19161214</td>\n      <td>4066625</td>\n    </tr>\n    <tr>\n      <th>rec-4405-org</th>\n      <td>charles</td>\n      <td>green</td>\n      <td>38</td>\n      <td>salkauskas crescent</td>\n      <td>kela</td>\n      <td>dapto</td>\n      <td>4566</td>\n      <td>nsw</td>\n      <td>19480930</td>\n      <td>4365168</td>\n    </tr>\n    <tr>\n      <th>rec-1288-org</th>\n      <td>vanessa</td>\n      <td>parr</td>\n      <td>905</td>\n      <td>macquoid place</td>\n      <td>broadbridge manor</td>\n      <td>south grafton</td>\n      <td>2135</td>\n      <td>sa</td>\n      <td>19951119</td>\n      <td>9239102</td>\n    </tr>\n    <tr>\n      <th>rec-3585-org</th>\n      <td>mikayla</td>\n      <td>malloney</td>\n      <td>37</td>\n      <td>randwick road</td>\n      <td>avalind</td>\n      <td>hoppers crossing</td>\n      <td>4552</td>\n      <td>vic</td>\n      <td>19860208</td>\n      <td>7207688</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA, dfB = load_febrl4()\n",
    "\n",
    "dfA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this linkage we will **not** use the social security id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:12.512706952Z",
     "start_time": "2023-05-17T03:40:12.496886636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['given_name', 'surname', 'street_number', 'address_1', 'address_2',\n       'suburb', 'postcode', 'state', 'date_of_birth', 'soc_sec_id'],\n      dtype='object')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use `StringIO` buffers instead of files. Let's dump the data from the pandas dataframe into a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:40.416870720Z",
     "start_time": "2023-05-17T03:40:40.392278049Z"
    }
   },
   "outputs": [],
   "source": [
    "a_csv = io.StringIO()\n",
    "dfA.to_csv(a_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linkage Schema Definition\n",
    "\n",
    "A hashing schema instructs `clkhash` how to treat each feature when encoding a CLK. \n",
    "\n",
    "The linkage schema below details a 1024 bit encoding using equally weighted features. Most features are encoding using bigrams although the postcode and date of birth use unigrams. The schema specifies to ignore the columns `'rec_id'` and `'soc_sec_id'`.\n",
    "\n",
    "A detailed description of the linkage schema can be found in the [documentation](http://clkhash.readthedocs.io/en/latest/schema.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:41.398701611Z",
     "start_time": "2023-05-17T03:40:41.356591303Z"
    }
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    Ignore('rec_id'),\n",
    "    StringSpec('given_name', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('surname', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('street_number', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    StringSpec('address_1', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('address_2', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('suburb', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('postcode', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300))),\n",
    "    StringSpec('state', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(300))),\n",
    "    IntegerSpec('date_of_birth', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(300), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    Ignore('soc_sec_id')\n",
    "]\n",
    "\n",
    "schema = Schema(fields, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the data\n",
    "\n",
    "We can now encode our PII data from the CSV file using our defined schema. We must provide a *secret* to this command - this secret has to be used by both parties hashing data. For this toy example we will use the secret `\"secret\"`, for real data, make sure that the key contains enough entropy, as knowledge of this secret is sufficient to reconstruct the PII information from a CLK! \n",
    "\n",
    "Also, **do not share this secret with anyone, except the other participating party.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:42.243783873Z",
     "start_time": "2023-05-17T03:40:42.197708371Z"
    }
   },
   "outputs": [],
   "source": [
    "secret = 'secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:43.630776130Z",
     "start_time": "2023-05-17T03:40:43.078030991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating CLKs: 5.00kclk [00:00, 9.09kclk/s, mean=944, std=14.4]\n"
     ]
    }
   ],
   "source": [
    "hashed_data_a = clk.generate_clk_from_csv(a_csv, secret, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output\n",
    "\n",
    "clkhash has encoded the PII, creating a Cryptographic Longterm Key for each entity. The output of `generate_clk_from_csv` shows that the mean popcount is quite high, more than 900 out of 1024 bits are set on average which can affect accuracy.\n",
    "\n",
    "We can control the popcount by adjusting the [strategy](https://clkhash.readthedocs.io/en/latest/schema.html#strategies). There are currently two different strategies implemented in the library:\n",
    "\n",
    "- `BitsPerToken`: each token of a feature's value is inserted into the encoding `bits_per_token` times. Increasing `bits_per_token` will give the corresponding feature more importance in comparisons, decreasing `bits_per_token` will de-emphasise columns which are less suitable for linkage (e.g. information that changes frequently). The `BitsPerToken` strategy is set with the `strategy=BitsPerTokenStrategy(bits_per_token=30)` argument for a feature's `FieldHashingProperties`.\n",
    "- `BitsPerFeature`: In this strategy we always insert a fixed number of bits into the CLK for a feature, irrespective of the number of tokens. This strategy is set with the `strategy=BitsPerFeatureStrategy(bits_per_feature=100)` argument for a feature's `FieldHashingProperties`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will reduce the value of `bits_per_feature` for address related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:47.011515297Z",
     "start_time": "2023-05-17T03:40:46.645286968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating CLKs: 5.00kclk [00:00, 14.0kclk/s, mean=696, std=22.7]\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    Ignore('rec_id'),\n",
    "    StringSpec('given_name', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(200))),\n",
    "    StringSpec('surname', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(200))),\n",
    "    IntegerSpec('street_number', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(100), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    StringSpec('address_1', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(100))),\n",
    "    StringSpec('address_2', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(100))),\n",
    "    StringSpec('suburb', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(100))),\n",
    "    IntegerSpec('postcode', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(100))),\n",
    "    StringSpec('state', FieldHashingProperties(comparator=NgramComparison(2), strategy=BitsPerFeatureStrategy(100))),\n",
    "    IntegerSpec('date_of_birth', FieldHashingProperties(comparator=NgramComparison(1, True), strategy=BitsPerFeatureStrategy(200), missing_value=MissingValueSpec(sentinel=''))),\n",
    "    Ignore('soc_sec_id')\n",
    "]\n",
    "\n",
    "schema = Schema(fields, 1024)\n",
    "clks_a = clk.generate_clk_from_csv(a_csv, secret, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each CLK is represented by a bitarray but can be serialized in a compact, JSON friendly base64 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:48.517468608Z",
     "start_time": "2023-05-17T03:40:48.510273185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "bitarray('1111111100101100001100011011110111100111001111111000111110010100011101111111111110111000110111111110111101011111111001011111011110111011101111001101011101100111101110001101101101010011001100110011010111110011010100101010111011111100101000111111101101111011100011100111110011110110110011110001010101101011011111111011011111110101100110010101111101111111101110001111110111111101010111100101110111100110111110100100110001100010110110111101101111011010111111110011110100101010111111110111011111100110111011111100001011111100011110000101010111111011101111011110110110001000100111111111111011101111101100111110111111011011001111100011111110111110100101101001000100011110101001000010101001110110111111111001111111111111010101011001110110101010110101100110110111000111111110111111000010111111000111110011111000100101111111111011111001111100011001101000110010111110111010001111111101110100101110001111001011111011111111011010110011011011001011010101011111111011011111110101111001101111010101111111011101111010001101110011101110111101')\n",
      "serialized:\n",
      "/ywxvec/j5R3/7jf71/l97u812e421MzNfNSrvyj+3uOfPbPFWt/t/WZX3+4/f1eXeb6TGLb29r/PSr/d+bvwvx4Vfu97Yif/u+z79s+P76WkR6kKnb/n/9VnarWbcf78L8fPiX/vnxmjL7o/3S48vv9rNstV/t/Xm9X93o3O70=\n"
     ]
    }
   ],
   "source": [
    "print(\"original:\")\n",
    "print(clks_a[0])\n",
    "print(\"serialized:\")\n",
    "print(serialize_bitarray(clks_a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash data set B\n",
    "\n",
    "Now we hash the second dataset using the same keys and same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:40:57.818863127Z",
     "start_time": "2023-05-17T03:40:57.483671162Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating CLKs: 5.00kclk [00:00, 18.9kclk/s, mean=687, std=30.4]\n"
     ]
    }
   ],
   "source": [
    "b_csv = io.StringIO()\n",
    "dfB.to_csv(b_csv)\n",
    "clks_b = clkhash.clk.generate_clk_from_csv(b_csv, secret, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:01.204284998Z",
     "start_time": "2023-05-17T03:41:01.194538481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clks_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matches between the two sets of CLKs\n",
    "\n",
    "We have generated two sets of CLKs which represent entity information in a privacy-preserving way. The more similar two CLKs are, the more likely it is that they represent the same entity.\n",
    "\n",
    "For this task we will use [anonlink](https://github.com/data61/anonlink), a Python (and optimised C++) implementation of anonymous linkage using CLKs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `anonlink` we find the candidate pairs - which is all possible pairs above the given `threshold`. Then we solve for the most likely mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:02.680045532Z",
     "start_time": "2023-05-17T03:41:02.668906952Z"
    }
   },
   "outputs": [],
   "source": [
    "import anonlink\n",
    "\n",
    "def mapping_from_clks(clks_a, clks_b, threshold):\n",
    "    results_candidate_pairs = anonlink.candidate_generation.find_candidate_pairs(\n",
    "            [clks_a, clks_b],\n",
    "            anonlink.similarities.dice_coefficient,\n",
    "            threshold\n",
    "    )\n",
    "    solution = anonlink.solving.greedy_solve(results_candidate_pairs)\n",
    "    print('Found {} matches'.format(len(solution)))\n",
    "    # each entry in `solution` looks like this: '((0, 4039), (1, 2689))'.\n",
    "    # The format is ((dataset_id, row_id), (dataset_id, row_id))\n",
    "    # As we only have two parties in this example, we can remove the dataset_ids.\n",
    "    # Also, turning the solution into a set will make it easier to assess the\n",
    "    # quality of the matching.\n",
    "    return set((a, b) for ((_, a), (_, b)) in solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:03.309061956Z",
     "start_time": "2023-05-17T03:41:03.168042726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4049 matches\n"
     ]
    }
   ],
   "source": [
    "found_matches = mapping_from_clks(clks_a, clks_b, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate matching quality\n",
    "Let's investigate some of those matches and the overall matching quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the febrl4 datasets contain record ids which tell us the correct linkages. Using this information we are able to create a set of the true matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:04.558424155Z",
     "start_time": "2023-05-17T03:41:04.547506008Z"
    }
   },
   "outputs": [],
   "source": [
    "# rec_id in dfA has the form 'rec-1070-org'. We only want the number. Additionally, as we are \n",
    "# interested in the position of the records, we create a new index which contains the row numbers.\n",
    "dfA_ = dfA.rename(lambda x: x[4:-4], axis='index').reset_index()\n",
    "dfB_ = dfB.rename(lambda x: x[4:-6], axis='index').reset_index()\n",
    "# now we can merge dfA_ and dfB_ on the record_id.\n",
    "a = pd.DataFrame({'ida': dfA_.index, 'rec_id': dfA_['rec_id']})\n",
    "b = pd.DataFrame({'idb': dfB_.index, 'rec_id': dfB_['rec_id']})\n",
    "dfj = a.merge(b, on='rec_id', how='inner').drop(columns=['rec_id'])\n",
    "# and build a set of the corresponding row numbers.\n",
    "true_matches = set((row[0], row[1]) for row in dfj.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:05.791767467Z",
     "start_time": "2023-05-17T03:41:05.789270120Z"
    }
   },
   "outputs": [],
   "source": [
    "def describe_matching_quality(found_matches, show_examples=False):\n",
    "    if show_examples:\n",
    "        print('idx_a, idx_b,     rec_id_a,       rec_id_b')\n",
    "        print('---------------------------------------------')\n",
    "        for a_i, b_i in itertools.islice(found_matches, 10):\n",
    "            print('{:4d}, {:5d}, {:>11}, {:>14}'.format(a_i+1, b_i+1, a.iloc[a_i]['rec_id'], b.iloc[b_i]['rec_id']))\n",
    "        print('---------------------------------------------')\n",
    "        \n",
    "    tp = len(found_matches & true_matches)\n",
    "    fp = len(found_matches - true_matches)\n",
    "    fn = len(true_matches - found_matches)\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    print('Precision: {:.3f}, Recall: {:.3f}'.format(precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:06.463210877Z",
     "start_time": "2023-05-17T03:41:06.457010387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_a, idx_b,     rec_id_a,       rec_id_b\n",
      "---------------------------------------------\n",
      "2560,  4916,         640,            640\n",
      "2127,  3079,        1102,           1102\n",
      "4776,   154,        4508,           4508\n",
      "2100,  4028,        4488,           4488\n",
      "3282,   736,         287,            287\n",
      " 951,  3116,        3925,           3925\n",
      "4361,   586,          65,             65\n",
      "2935,  4155,        3050,           3050\n",
      "1026,  3555,          47,             47\n",
      "2841,  2162,        1689,           1689\n",
      "---------------------------------------------\n",
      "Precision: 1.000, Recall: 0.810\n"
     ]
    }
   ],
   "source": [
    "describe_matching_quality(found_matches, show_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision tells us about how many of the found matches are actual matches. The score of 1.0 means that we did perfectly in this respect, however, recall, the measure of how many of the actual matches were correctly identified, is quite low with only 81%.\n",
    "\n",
    "Let's go back to the mapping calculation (`mapping_from_clks`) an reduce the value for `threshold` to `0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T03:41:07.680194280Z",
     "start_time": "2023-05-17T03:41:07.529745508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4962 matches\n",
      "Precision: 1.000, Recall: 0.992\n"
     ]
    }
   ],
   "source": [
    "found_matches = mapping_from_clks(clks_a, clks_b, 0.8)\n",
    "describe_matching_quality(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Great, for this threshold value we get a precision of 100% and a recall of 99.2%. \n",
    "\n",
    "The explanation is that when the information about an entity differs slightly in the two datasets (e.g. spelling errors, abbrevations, missing values, ...) then the corresponding CLKs will differ in some number of bits as well. It is important to choose an appropriate threshold for the amount of perturbations present in the data (a threshold of 0.72 and below generates an almost perfect mapping with little mistakes).\n",
    "\n",
    "This concludes the tutorial. Feel free to go back to the CLK generation and experiment on how different setting will affect the matching quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
