{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for CLI tool `clkhash`\n",
    "\n",
    "For this tutorial we are going to process a data set for private linkage with clkhash using the command line tool `clkutil`. Note you can also use the [Python API](./tutorial_api.ipynb).\n",
    "\n",
    "The Python package `recordlinkage` has a [tutorial](http://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html) linking data sets in the clear, we will try duplicate that in a privacy preserving setting.\n",
    "\n",
    "First install clkhash, recordlinkage and a few data science tools (pandas and numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -U clkhash recordlinkage numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "from recordlinkage.datasets import load_febrl4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First we have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-1070-org</th>\n",
       "      <td>michaela</td>\n",
       "      <td>neumann</td>\n",
       "      <td>8</td>\n",
       "      <td>stanley street</td>\n",
       "      <td>miami</td>\n",
       "      <td>winston hills</td>\n",
       "      <td>4223</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19151111</td>\n",
       "      <td>5304218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1016-org</th>\n",
       "      <td>courtney</td>\n",
       "      <td>painter</td>\n",
       "      <td>12</td>\n",
       "      <td>pinkerton circuit</td>\n",
       "      <td>bega flats</td>\n",
       "      <td>richlands</td>\n",
       "      <td>4560</td>\n",
       "      <td>vic</td>\n",
       "      <td>19161214</td>\n",
       "      <td>4066625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-4405-org</th>\n",
       "      <td>charles</td>\n",
       "      <td>green</td>\n",
       "      <td>38</td>\n",
       "      <td>salkauskas crescent</td>\n",
       "      <td>kela</td>\n",
       "      <td>dapto</td>\n",
       "      <td>4566</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19480930</td>\n",
       "      <td>4365168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1288-org</th>\n",
       "      <td>vanessa</td>\n",
       "      <td>parr</td>\n",
       "      <td>905</td>\n",
       "      <td>macquoid place</td>\n",
       "      <td>broadbridge manor</td>\n",
       "      <td>south grafton</td>\n",
       "      <td>2135</td>\n",
       "      <td>sa</td>\n",
       "      <td>19951119</td>\n",
       "      <td>9239102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-3585-org</th>\n",
       "      <td>mikayla</td>\n",
       "      <td>malloney</td>\n",
       "      <td>37</td>\n",
       "      <td>randwick road</td>\n",
       "      <td>avalind</td>\n",
       "      <td>hoppers crossing</td>\n",
       "      <td>4552</td>\n",
       "      <td>vic</td>\n",
       "      <td>19860208</td>\n",
       "      <td>7207688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             given_name   surname street_number            address_1  \\\n",
       "rec_id                                                                 \n",
       "rec-1070-org   michaela   neumann             8       stanley street   \n",
       "rec-1016-org   courtney   painter            12    pinkerton circuit   \n",
       "rec-4405-org    charles     green            38  salkauskas crescent   \n",
       "rec-1288-org    vanessa      parr           905       macquoid place   \n",
       "rec-3585-org    mikayla  malloney            37        randwick road   \n",
       "\n",
       "                      address_2            suburb postcode state  \\\n",
       "rec_id                                                             \n",
       "rec-1070-org              miami     winston hills     4223   nsw   \n",
       "rec-1016-org         bega flats         richlands     4560   vic   \n",
       "rec-4405-org               kela             dapto     4566   nsw   \n",
       "rec-1288-org  broadbridge manor     south grafton     2135    sa   \n",
       "rec-3585-org            avalind  hoppers crossing     4552   vic   \n",
       "\n",
       "             date_of_birth soc_sec_id  \n",
       "rec_id                                 \n",
       "rec-1070-org      19151111    5304218  \n",
       "rec-1016-org      19161214    4066625  \n",
       "rec-4405-org      19480930    4365168  \n",
       "rec-1288-org      19951119    9239102  \n",
       "rec-3585-org      19860208    7207688  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA, dfB = load_febrl4()\n",
    "\n",
    "dfA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for computing this linkage we will **not** use the social security id column or the `rec_id` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['given_name', 'surname', 'street_number', 'address_1', 'address_2',\n",
       "       'suburb', 'postcode', 'state', 'date_of_birth', 'soc_sec_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dfA.to_csv('PII_a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Schema Definition\n",
    "\n",
    "A hashing schema instructs clkhash how to treat each column for generating CLKs. A detailed description of the hashing schema can be found in the [api docs](http://clkhash.readthedocs.io/en/latest/schema.html). We will ignore the columns 'rec_id' and 'soc_sec_id' for CLK generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"version\": 2,\r\n",
      "  \"clkConfig\": {\r\n",
      "    \"l\": 1024,\r\n",
      "    \"kdf\": {\r\n",
      "      \"type\": \"HKDF\",\r\n",
      "      \"hash\": \"SHA256\",\r\n",
      "        \"info\": \"c2NoZW1hX2V4YW1wbGU=\",\r\n",
      "        \"salt\": \"SCbL2zHNnmsckfzchsNkZY9XoHk96P/G5nUBrM7ybymlEFsMV6PAeDZCNp3rfNUPCtLDMOGQHG4pCQpfhiHCyA==\",\r\n",
      "        \"keySize\": 64\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"features\": [\r\n",
      "    {\r\n",
      "      \"identifier\": \"rec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"given_name\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 500}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"surname\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 500}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"street_number\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\": 200}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_1\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_2\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"suburb\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"postcode\",\r\n",
      "      \"format\": { \"type\": \"integer\", \"minimum\": 100, \"maximum\": 9999 },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"state\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 3 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"date_of_birth\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  200}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"soc_sec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "%cat _static/febrl_schema_v2_overweight.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the schema\n",
    "\n",
    "The command line tool can check that the linkage schema is valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mschema is valid\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!clkutil validate-schema _static/febrl_schema_v2_overweight.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash the data\n",
    "\n",
    "We can now hash our Personally Identifiable Information (PII) data from the CSV file using our defined linkage schema. We must provide two *secret keys* to this command - these keys have to be used by both parties hashing data. For this toy example we will use the keys _'key1'_ and _'key2'_, for real data, make sure that the keys contain enough entropy, as knowledge of these keys is sufficient to reconstruct the PII information from a CLK! Also, **do not share these keys with anyone, except the other participating party.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00k/5.00k [00:00<00:00, 1.52kclk/s, mean=925, std=11.5]\n",
      "\u001b[31mCLK data written to clks_a.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil hash PII_a.csv key1 key2 _static/febrl_schema_v2_overweight.json clks_a.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output\n",
    "\n",
    "clkhash has hashed the PII, creating a Cryptographic Longterm Key for each entity. The stats output shows that the mean popcount (number of bits set) is quite high (925 out of 1024) which can effect accuracy.\n",
    "\n",
    "To reduce the popcount you can modify the individual _'numBits'_ values for the different fields. It allows to tune the contribution of a column to the CLK. This can be used to de-emphasise columns which are less suitable for linkage (e.g. information that changes frequently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will reduce the value of *numBits* for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"version\": 2,\r\n",
      "  \"clkConfig\": {\r\n",
      "    \"l\": 1024,\r\n",
      "    \"kdf\": {\r\n",
      "      \"type\": \"HKDF\",\r\n",
      "      \"hash\": \"SHA256\",\r\n",
      "        \"info\": \"c2NoZW1hX2V4YW1wbGU=\",\r\n",
      "        \"salt\": \"SCbL2zHNnmsckfzchsNkZY9XoHk96P/G5nUBrM7ybymlEFsMV6PAeDZCNp3rfNUPCtLDMOGQHG4pCQpfhiHCyA==\",\r\n",
      "        \"keySize\": 64\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"features\": [\r\n",
      "    {\r\n",
      "      \"identifier\": \"rec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"given_name\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 100}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"surname\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 100}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"street_number\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\": 100}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_1\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  100} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_2\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  100} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"suburb\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  100} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"postcode\",\r\n",
      "      \"format\": { \"type\": \"integer\", \"minimum\": 100, \"maximum\": 9999 },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  100} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"state\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 3 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 200} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"date_of_birth\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  100}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"soc_sec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "%cat _static/febrl_schema_v2_reduced.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00k/5.00k [00:00<00:00, 7.46kclk/s, mean=633, std=14.1]\n",
      "\u001b[31mCLK data written to clks_a.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil hash PII_a.csv key1 key2 _static/febrl_schema_v2_reduced.json clks_a.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will modify the `numBits` values again, this time de-emphasising the contribution of the address related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"version\": 2,\r\n",
      "  \"clkConfig\": {\r\n",
      "    \"l\": 1024,\r\n",
      "    \"kdf\": {\r\n",
      "      \"type\": \"HKDF\",\r\n",
      "      \"hash\": \"SHA256\",\r\n",
      "        \"info\": \"c2NoZW1hX2V4YW1wbGU=\",\r\n",
      "        \"salt\": \"SCbL2zHNnmsckfzchsNkZY9XoHk96P/G5nUBrM7ybymlEFsMV6PAeDZCNp3rfNUPCtLDMOGQHG4pCQpfhiHCyA==\",\r\n",
      "        \"keySize\": 64\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"features\": [\r\n",
      "    {\r\n",
      "      \"identifier\": \"rec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"given_name\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 200}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"surname\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\", \"maxLength\": 64 },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\": 150}, \"hash\": {\"type\": \"doubleHash\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"street_number\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\": 50}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_1\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  50} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"address_2\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  50} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"suburb\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\": \"utf-8\" },\r\n",
      "      \"hashing\": { \"ngram\": 2, \"strategy\": {\"numBits\":  50} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"postcode\",\r\n",
      "      \"format\": { \"type\": \"integer\", \"minimum\": 50, \"maximum\": 9999 },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  50} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"state\",\r\n",
      "      \"format\": { \"type\": \"string\", \"encoding\":  \"utf-8\"},\r\n",
      "      \"hashing\": {\"ngram\": 2, \"positional\": true, \"strategy\": {\"numBits\": 75}, \"missingValue\": {\"sentinel\":  \"\", \"replaceWith\": \"na\"}\r\n",
      "      }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"date_of_birth\",\r\n",
      "      \"format\": { \"type\": \"integer\" },\r\n",
      "      \"hashing\": { \"ngram\": 1, \"positional\": true, \"strategy\": {\"numBits\":  100}, \"missingValue\": {\"sentinel\": \"\"} }\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"identifier\": \"soc_sec_id\",\r\n",
      "      \"ignored\": true\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "%cat _static/febrl_schema_v2_final.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00k/5.00k [00:00<00:00, 7.90kclk/s, mean=543, std=13.1]\n",
      "\u001b[31mCLK data written to clks_a.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil hash PII_a.csv key1 key2 _static/febrl_schema_v2_final.json clks_a.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now approximately half the bits are set in each CLK. \n",
    "\n",
    "Each CLK is serialized in a JSON friendly base64 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GnoZ2WzD14Mwib5NaXBL05xeaj6h8Hq06Xza8KcaHWWWrHygGXmNHuRW9tOd314KKvmnu5bzYELAIstOMPcPYEY97CjZJqrAg8S/CwL3PdHdtplV1ao2wJ+Ve53iXg4Vd988b3mhTsIQQd+7Xj2QXdpVSsSWNkmkX8uVKYI7nv0='"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have jq tool installed:\n",
    "#!jq .clks[0] clks_a.json\n",
    "\n",
    "import json\n",
    "json.load(open('clks_a.json'))['clks'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash data set B\n",
    "\n",
    "Now we hash the second dataset using the same keys and same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00k/5.00k [00:00<00:00, 8.08kclk/s, mean=542, std=15.5]\n",
      "\u001b[31mCLK data written to clks_b.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dfB.to_csv('PII_b.csv')\n",
    "\n",
    "!clkutil hash PII_b.csv key1 key2 _static/febrl_schema_v2_final.json clks_b.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matches between the two sets of CLKs\n",
    "\n",
    "We have generated two sets of CLKs which represent entity information in a privacy-preserving way. The more similar two CLKs are, the more likely it is that they represent the same entity.\n",
    "\n",
    "For this task we will use the entity service, which is provided by Data61. \n",
    "The necessary steps are as follows:\n",
    "- The analyst creates a new project with the output type 'mapping'. They will receive a set of credentials from the server.\n",
    "- The analyst then distributes the `update_tokens` to the participating data providers.\n",
    "- The data providers then individually upload their respective CLKs.\n",
    "- The analyst can create *runs* with various thresholds (and other settings)\n",
    "- After the entity service successfully computed the mapping, it can be accessed by providing the `result_token`\n",
    "\n",
    "First we check the status of an entity service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"project_count\": 1226, \"rate\": 503702171, \"status\": \"ok\"}\r\n"
     ]
    }
   ],
   "source": [
    "SERVER = 'https://testing.es.data61.xyz'\n",
    "\n",
    "!clkutil status --server={SERVER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyst creates a new project on the entity service by providing the hashing schema and result type. The server returns a set of credentials which provide access to the further steps for project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mProject created\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!clkutil create-project --server={SERVER} --schema schema.json --output credentials.json --type \"mapping\" --name \"tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned credentials contain a \n",
    "- `project_id`, which identifies the project\n",
    "- `result_token`, which gives access to the mapping result, once computed\n",
    "- `upload_tokens`, one for each provider, allows uploading CLKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"project_id\": \"9cb00ea7424692d75b3e2b06d87826b1ef5eaf707d8b710e\",\r\n",
      "    \"result_token\": \"9b5b3eae23569ae200c87af765e87f951ce1bc6e30f27610\",\r\n",
      "    \"update_tokens\": [\r\n",
      "        \"9799e866dd51979d78ccae11b1741a777029e347dbf19d48\",\r\n",
      "        \"c8518faea21267785d4e998a8060c242ba5a6a575aeaa32f\"\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "credentials = json.load(open('credentials.json', 'rt'))\n",
    "!python -m json.tool credentials.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the CLKs to the entity service\n",
    "Each party individually uploads its respective CLKs to the entity service. They need to provide the `resource_id`, which identifies the correct mapping, and an `update_token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!clkutil upload \\\n",
    "       --project=\"{credentials['project_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][0]}\" \\\n",
    "        --output \"upload_a.json\" \\\n",
    "        --server=\"{SERVER}\" \\\n",
    "       \"clks_a.json\"\n",
    "    \n",
    "!clkutil upload \\\n",
    "       --project=\"{credentials['project_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][1]}\" \\\n",
    "        --output \"upload_b.json\" \\\n",
    "        --server=\"{SERVER}\" \\\n",
    "       \"clks_b.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the CLK data has been uploaded the analyst can create one or more *runs*. Here we will start by calculating a mapping with a threshold of `0.9`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntity Matching Server: https://testing.es.data61.xyz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!clkutil create --verbose  \\\n",
    "    --server=\"{SERVER}\" \\\n",
    "    --output \"run_info.json\" \\\n",
    "    --threshold=0.9 \\\n",
    "    --project=\"{credentials['project_id']}\" \\\n",
    "    --apikey=\"{credentials['result_token']}\" \\\n",
    "    --name=\"CLI tutorial run A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'CLI tutorial run A',\n",
       " 'notes': 'Run created by clkhash 0.12.2.dev0',\n",
       " 'run_id': '2909a60505a0fd93c925c5e02088c22aa5a4731d03522ebc',\n",
       " 'threshold': 0.9}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_info = json.load(open('run_info.json', 'rt'))\n",
    "run_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now after some delay (depending on the size) we can fetch the results. This can be done with clkutil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mState: running\n",
      "Stage (2/3): compute similarity scores\u001b[0m\n",
      "\u001b[31mState: running\n",
      "Stage (2/3): compute similarity scores\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mDownloading result\u001b[0m\n",
      "\u001b[31mReceived result\u001b[0m\n",
      "The service linked 3392 entities.\n"
     ]
    }
   ],
   "source": [
    "!clkutil results --watch \\\n",
    "        --project=\"{credentials['project_id']}\" \\\n",
    "        --apikey=\"{credentials['result_token']}\" \\\n",
    "        --run=\"{run_info['run_id']}\" \\\n",
    "        --server=\"{SERVER}\" \\\n",
    "        --output results.txt\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    str_mapping = json.load(f)['mapping']\n",
    "    mapping = {int(k): int(v) for k,v in str_mapping.items()}\n",
    "print('The service linked {} entities.'.format(len(mapping)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate some of those matches and the overall matching quality. In this case we have the ground truth so we can compute the precision, recall, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PII_a.csv', 'rt') as f:\n",
    "    a_raw = f.readlines()\n",
    "with open('PII_b.csv', 'rt') as f:\n",
    "    b_raw = f.readlines()\n",
    "\n",
    "num_entities = len(b_raw) - 1\n",
    "\n",
    "def describe_accuracy(mapping):\n",
    "    print('idx_a, idx_b, rec_id_a, rec_id_b')\n",
    "    print('--------------------------------')\n",
    "    for a_i in range(10):\n",
    "        if a_i in mapping:\n",
    "            a_data = a_raw[a_i + 1].split(',')\n",
    "            b_data = b_raw[mapping[a_i] + 1].split(',')\n",
    "            print('{}, {}, {}, {}'.format(a_i+1, mapping[a_i]+1, a_data[0], b_data[0]))\n",
    "\n",
    "    TP = 0; FP = 0; TN = 0; FN = 0\n",
    "    for a_i in range(num_entities):\n",
    "        if a_i in mapping:\n",
    "            if a_raw[a_i + 1].split(',')[0].split('-')[1] == b_raw[mapping[a_i] + 1].split(',')[0].split('-')[1]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "                # as we only report one mapping for each element in PII_a, \n",
    "                # then a wrong mapping is not only a false positive, but \n",
    "                # also a false negative, as we won't report the true mapping.\n",
    "                FN += 1 \n",
    "        else:\n",
    "            FN += 1 # every element in PII_a has a partner in PII_b\n",
    "\n",
    "    print('--------------------------------')\n",
    "    print('Precision: {}, Recall: {}, Accuracy: {}'.format(TP/(TP+FP), TP/(TP+FN), (TP+TN)/(TP+TN+FP+FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_a, idx_b, rec_id_a, rec_id_b\n",
      "--------------------------------\n",
      "2, 2751, rec-1016-org, rec-1016-dup-0\n",
      "3, 4657, rec-4405-org, rec-4405-dup-0\n",
      "4, 4120, rec-1288-org, rec-1288-dup-0\n",
      "5, 3307, rec-3585-org, rec-3585-dup-0\n",
      "6, 2306, rec-298-org, rec-298-dup-0\n",
      "7, 3945, rec-1985-org, rec-1985-dup-0\n",
      "8, 993, rec-2404-org, rec-2404-dup-0\n",
      "9, 4613, rec-1473-org, rec-1473-dup-0\n",
      "10, 3630, rec-453-org, rec-453-dup-0\n",
      "--------------------------------\n",
      "Precision: 1.0, Recall: 0.6784, Accuracy: 0.6784\n"
     ]
    }
   ],
   "source": [
    "describe_accuracy(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision tells us about how many of the found matches are actual matches. The score of 1.0 means that we did perfectly in this respect, however, **recall**, the measure of how many of the actual matches were correctly identified, is quite low with only 67%.\n",
    "\n",
    "Let's go back and create another mapping with a `threshold` value of `0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntity Matching Server: https://testing.es.data61.xyz\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mDownloading result\u001b[0m\n",
      "\u001b[31mReceived result\u001b[0m\n",
      "The service linked 4409 entities.\n",
      "idx_a, idx_b, rec_id_a, rec_id_b\n",
      "--------------------------------\n",
      "2, 2751, rec-1016-org, rec-1016-dup-0\n",
      "3, 4657, rec-4405-org, rec-4405-dup-0\n",
      "4, 4120, rec-1288-org, rec-1288-dup-0\n",
      "5, 3307, rec-3585-org, rec-3585-dup-0\n",
      "6, 2306, rec-298-org, rec-298-dup-0\n",
      "7, 3945, rec-1985-org, rec-1985-dup-0\n",
      "8, 993, rec-2404-org, rec-2404-dup-0\n",
      "9, 4613, rec-1473-org, rec-1473-dup-0\n",
      "10, 3630, rec-453-org, rec-453-dup-0\n",
      "--------------------------------\n",
      "Precision: 1.0, Recall: 0.8818, Accuracy: 0.8818\n"
     ]
    }
   ],
   "source": [
    "!clkutil create --verbose  \\\n",
    "    --server=\"{SERVER}\" \\\n",
    "    --output \"run_info.json\" \\\n",
    "    --threshold=0.8 \\\n",
    "    --project=\"{credentials['project_id']}\" \\\n",
    "    --apikey=\"{credentials['result_token']}\" \\\n",
    "    --name=\"CLI tutorial run B\"\n",
    "\n",
    "run_info = json.load(open('run_info.json', 'rt'))\n",
    "\n",
    "!clkutil results --watch \\\n",
    "        --project=\"{credentials['project_id']}\" \\\n",
    "        --apikey=\"{credentials['result_token']}\" \\\n",
    "        --run=\"{run_info['run_id']}\" \\\n",
    "        --server=\"{SERVER}\" \\\n",
    "        --output results.txt\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    str_mapping = json.load(f)['mapping']\n",
    "    mapping = {int(k): int(v) for k,v in str_mapping.items()}\n",
    "\n",
    "print('The service linked {} entities.'.format(len(mapping)))\n",
    "describe_accuracy(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, for this threshold value we get a precision of 100% and a recall of 88%. \n",
    "\n",
    "The explanation is that when the information about an entity differs slightly in the two datasets (e.g. spelling errors, abbrevations, missing values, ...) then the corresponding CLKs will differ in some number of bits as well. For the datasets in this tutorial the perturbations are such that only 67% of the derived CLK pairs overlap more than 90% (the first threshold). Whereas 88% of all matching pairs overlap more than 80%.\n",
    "\n",
    "If we keep reducing the threshold value, then we will start to observe mistakes in the found matches -- the precision decreases. But at the same time the recall value will keep increasing for a while, as a lower threshold allows for more of the actual matches to be found, e.g.: for threshold 0.7, we get precision: 99% and recall: 98%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntity Matching Server: https://testing.es.data61.xyz\u001b[0m\n",
      "\u001b[31mState: running\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mState: completed\n",
      "Stage (3/3): compute output\u001b[0m\n",
      "\u001b[31mDownloading result\u001b[0m\n",
      "\u001b[31mReceived result\u001b[0m\n",
      "The service linked 4943 entities.\n",
      "idx_a, idx_b, rec_id_a, rec_id_b\n",
      "--------------------------------\n",
      "1, 1450, rec-1070-org, rec-1070-dup-0\n",
      "2, 2751, rec-1016-org, rec-1016-dup-0\n",
      "3, 4657, rec-4405-org, rec-4405-dup-0\n",
      "4, 4120, rec-1288-org, rec-1288-dup-0\n",
      "5, 3307, rec-3585-org, rec-3585-dup-0\n",
      "6, 2306, rec-298-org, rec-298-dup-0\n",
      "7, 3945, rec-1985-org, rec-1985-dup-0\n",
      "8, 993, rec-2404-org, rec-2404-dup-0\n",
      "9, 4613, rec-1473-org, rec-1473-dup-0\n",
      "10, 3630, rec-453-org, rec-453-dup-0\n",
      "--------------------------------\n",
      "Precision: 0.9997976937082743, Recall: 0.9884, Accuracy: 0.9882023595280944\n"
     ]
    }
   ],
   "source": [
    "!clkutil create --verbose  \\\n",
    "    --server=\"{SERVER}\" \\\n",
    "    --output \"run_info.json\" \\\n",
    "    --threshold=0.70 \\\n",
    "    --project=\"{credentials['project_id']}\" \\\n",
    "    --apikey=\"{credentials['result_token']}\" \\\n",
    "    --name=\"CLI tutorial run B\"\n",
    "\n",
    "run_info = json.load(open('run_info.json', 'rt'))\n",
    "\n",
    "!clkutil results --watch \\\n",
    "        --project=\"{credentials['project_id']}\" \\\n",
    "        --apikey=\"{credentials['result_token']}\" \\\n",
    "        --run=\"{run_info['run_id']}\" \\\n",
    "        --server=\"{SERVER}\" \\\n",
    "        --output results.txt\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    str_mapping = json.load(f)['mapping']\n",
    "    mapping = {int(k): int(v) for k,v in str_mapping.items()}\n",
    "\n",
    "print('The service linked {} entities.'.format(len(mapping)))\n",
    "describe_accuracy(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, reducing the threshold further will eventually lead to a decrease in both precision and recall. Thus it is important to choose an appropriate threshold for the amount of perturbations present in the data.\n",
    "\n",
    "Feel free to go back to the CLK generation and experiment on how different setting will affect the matching quality.\n",
    "\n",
    "Finally to remove the uploaded CLK data from the service delete the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "#!clkutil delete-project --project=\"{credentials['project_id']}\" \\\n",
    "#        --apikey=\"{credentials['result_token']}\" \\\n",
    "#        --run=\"{run_info['run_id']}\" \\\n",
    "#        --server=\"{SERVER}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
